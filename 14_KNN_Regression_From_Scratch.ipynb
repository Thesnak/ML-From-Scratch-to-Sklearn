{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# k-Nearest Neighbors (k-NN) Regression From Scratch\n",
                "\n",
                "k-NN Regression predicts continuous values by averaging the target values of the k nearest neighbors.\n",
                "\n",
                "## Key Concepts:\n",
                "- **Instance-Based Learning**: No explicit training phase\n",
                "- **Distance Metrics**: Euclidean, Manhattan, etc.\n",
                "- **Averaging**: Prediction based on mean of k nearest neighbors\n",
                "- **Weighted Averaging**: Optional distance-weighted predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import make_regression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mathematical Foundation\n",
                "\n",
                "### Distance Metrics:\n",
                "\n",
                "**Euclidean Distance:**\n",
                "$$d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$\n",
                "\n",
                "### Prediction Rule (Simple Average):\n",
                "$$\\hat{y} = \\frac{1}{k} \\sum_{i=1}^{k} y_i$$\n",
                "\n",
                "### Prediction Rule (Weighted Average):\n",
                "$$\\hat{y} = \\frac{\\sum_{i=1}^{k} w_i y_i}{\\sum_{i=1}^{k} w_i}$$\n",
                "\n",
                "where $w_i = \\frac{1}{d_i + \\epsilon}$ (inverse distance weighting)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class KNNRegressor:\n",
                "    def __init__(self, k=3, distance_metric='euclidean', weights='uniform'):\n",
                "        \"\"\"\n",
                "        Initialize k-NN Regressor\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        k : int\n",
                "            Number of neighbors to consider (default=3)\n",
                "        distance_metric : str\n",
                "            Distance metric: 'euclidean' or 'manhattan'\n",
                "        weights : str\n",
                "            Weight function: 'uniform' or 'distance'\n",
                "        \"\"\"\n",
                "        self.k = k\n",
                "        self.distance_metric = distance_metric\n",
                "        self.weights = weights\n",
                "        self.X_train = None\n",
                "        self.y_train = None\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        \"\"\"\n",
                "        'Train' the model (just store the training data)\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        X : array-like, shape (n_samples, n_features)\n",
                "        y : array-like, shape (n_samples,)\n",
                "        \"\"\"\n",
                "        self.X_train = X\n",
                "        self.y_train = y\n",
                "        return self\n",
                "    \n",
                "    def _calculate_distance(self, x1, x2):\n",
                "        \"\"\"\n",
                "        Calculate distance between two points\n",
                "        \"\"\"\n",
                "        if self.distance_metric == 'euclidean':\n",
                "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
                "        elif self.distance_metric == 'manhattan':\n",
                "            return np.sum(np.abs(x1 - x2))\n",
                "        else:\n",
                "            raise ValueError(f\"Unknown distance metric: {self.distance_metric}\")\n",
                "    \n",
                "    def _predict_single(self, x):\n",
                "        \"\"\"\n",
                "        Predict value for a single sample\n",
                "        \"\"\"\n",
                "        # Calculate distances to all training samples\n",
                "        distances = np.array([self._calculate_distance(x, x_train) for x_train in self.X_train])\n",
                "        \n",
                "        # Get indices of k nearest neighbors\n",
                "        k_indices = np.argsort(distances)[:self.k]\n",
                "        \n",
                "        # Get values of k nearest neighbors\n",
                "        k_nearest_values = self.y_train[k_indices]\n",
                "        \n",
                "        # Calculate prediction based on weighting scheme\n",
                "        if self.weights == 'uniform':\n",
                "            # Simple average\n",
                "            return np.mean(k_nearest_values)\n",
                "        elif self.weights == 'distance':\n",
                "            # Weighted average (inverse distance)\n",
                "            k_distances = distances[k_indices]\n",
                "            # Add small epsilon to avoid division by zero\n",
                "            weights = 1 / (k_distances + 1e-10)\n",
                "            return np.sum(weights * k_nearest_values) / np.sum(weights)\n",
                "        else:\n",
                "            raise ValueError(f\"Unknown weights: {self.weights}\")\n",
                "    \n",
                "    def predict(self, X):\n",
                "        \"\"\"\n",
                "        Predict values for multiple samples\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        X : array-like, shape (n_samples, n_features)\n",
                "        \n",
                "        Returns:\n",
                "        --------\n",
                "        predictions : array, shape (n_samples,)\n",
                "        \"\"\"\n",
                "        return np.array([self._predict_single(x) for x in X])\n",
                "    \n",
                "    def score(self, X, y):\n",
                "        \"\"\"\n",
                "        Calculate R² score\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        X : array-like, shape (n_samples, n_features)\n",
                "        y : array-like, shape (n_samples,)\n",
                "        \n",
                "        Returns:\n",
                "        --------\n",
                "        r2_score : float\n",
                "        \"\"\"\n",
                "        y_pred = self.predict(X)\n",
                "        ss_res = np.sum((y - y_pred) ** 2)\n",
                "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
                "        return 1 - (ss_res / ss_tot)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Testing on Synthetic Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate synthetic data\n",
                "np.random.seed(42)\n",
                "X, y = make_regression(n_samples=200, n_features=1, noise=15, random_state=42)\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Standardize features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"Training samples: {X_train.shape[0]}\")\n",
                "print(f\"Test samples: {X_test.shape[0]}\")\n",
                "print(f\"Features: {X_train.shape[1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train k-NN Regressor\n",
                "knn = KNNRegressor(k=5)\n",
                "knn.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred_train = knn.predict(X_train_scaled)\n",
                "y_pred_test = knn.predict(X_test_scaled)\n",
                "\n",
                "# Calculate scores\n",
                "train_score = knn.score(X_train_scaled, y_train)\n",
                "test_score = knn.score(X_test_scaled, y_test)\n",
                "\n",
                "print(f\"\\nk-NN Regressor (k={knn.k})\")\n",
                "print(f\"Train R² Score: {train_score:.4f}\")\n",
                "print(f\"Test R² Score: {test_score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Comparison with Scikit-learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.neighbors import KNeighborsRegressor\n",
                "\n",
                "# Train sklearn k-NN\n",
                "sklearn_knn = KNeighborsRegressor(n_neighbors=5)\n",
                "sklearn_knn.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Compare scores\n",
                "sklearn_train_score = sklearn_knn.score(X_train_scaled, y_train)\n",
                "sklearn_test_score = sklearn_knn.score(X_test_scaled, y_test)\n",
                "\n",
                "print(\"\\nComparison:\")\n",
                "print(f\"{'Method':<20} {'Train R²':<12} {'Test R²':<12}\")\n",
                "print(\"-\" * 44)\n",
                "print(f\"{'Our k-NN':<20} {train_score:<12.4f} {test_score:<12.4f}\")\n",
                "print(f\"{'Sklearn k-NN':<20} {sklearn_train_score:<12.4f} {sklearn_test_score:<12.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Effect of k (Number of Neighbors)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different k values\n",
                "k_values = range(1, 31)\n",
                "train_scores = []\n",
                "test_scores = []\n",
                "\n",
                "for k in k_values:\n",
                "    knn = KNNRegressor(k=k)\n",
                "    knn.fit(X_train_scaled, y_train)\n",
                "    \n",
                "    train_scores.append(knn.score(X_train_scaled, y_train))\n",
                "    test_scores.append(knn.score(X_test_scaled, y_test))\n",
                "\n",
                "# Plot results\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(k_values, train_scores, 'o-', label='Train R²', linewidth=2)\n",
                "plt.plot(k_values, test_scores, 's-', label='Test R²', linewidth=2)\n",
                "plt.xlabel('k (Number of Neighbors)', fontsize=12)\n",
                "plt.ylabel('R² Score', fontsize=12)\n",
                "plt.title('k-NN Regression: Effect of k on Performance', fontsize=14)\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "# Find best k\n",
                "best_k = k_values[np.argmax(test_scores)]\n",
                "best_score = max(test_scores)\n",
                "print(f\"\\nBest k: {best_k}\")\n",
                "print(f\"Best Test R²: {best_score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Uniform vs Distance-Weighted Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare uniform vs distance weighting\n",
                "knn_uniform = KNNRegressor(k=5, weights='uniform')\n",
                "knn_distance = KNNRegressor(k=5, weights='distance')\n",
                "\n",
                "knn_uniform.fit(X_train_scaled, y_train)\n",
                "knn_distance.fit(X_train_scaled, y_train)\n",
                "\n",
                "uniform_score = knn_uniform.score(X_test_scaled, y_test)\n",
                "distance_score = knn_distance.score(X_test_scaled, y_test)\n",
                "\n",
                "print(\"\\nWeighting Scheme Comparison:\")\n",
                "print(f\"Uniform Weights:  {uniform_score:.4f}\")\n",
                "print(f\"Distance Weights: {distance_score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Visualization of Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train models with different k values\n",
                "knn_k1 = KNNRegressor(k=1)\n",
                "knn_k5 = KNNRegressor(k=5)\n",
                "knn_k15 = KNNRegressor(k=15)\n",
                "\n",
                "knn_k1.fit(X_train_scaled, y_train)\n",
                "knn_k5.fit(X_train_scaled, y_train)\n",
                "knn_k15.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Create smooth line for predictions\n",
                "X_line = np.linspace(X_train_scaled.min(), X_train_scaled.max(), 300).reshape(-1, 1)\n",
                "y_pred_k1 = knn_k1.predict(X_line)\n",
                "y_pred_k5 = knn_k5.predict(X_line)\n",
                "y_pred_k15 = knn_k15.predict(X_line)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(15, 5))\n",
                "\n",
                "for i, (k, y_pred, title) in enumerate([\n",
                "    (1, y_pred_k1, 'k=1 (High Variance)'),\n",
                "    (5, y_pred_k5, 'k=5 (Balanced)'),\n",
                "    (15, y_pred_k15, 'k=15 (High Bias)')\n",
                "]):\n",
                "    plt.subplot(1, 3, i+1)\n",
                "    plt.scatter(X_train_scaled, y_train, alpha=0.5, s=30, label='Training Data')\n",
                "    plt.plot(X_line, y_pred, 'r-', linewidth=2, label=f'k-NN (k={k})')\n",
                "    plt.xlabel('Feature', fontsize=11)\n",
                "    plt.ylabel('Target', fontsize=11)\n",
                "    plt.title(title, fontsize=12)\n",
                "    plt.legend(fontsize=10)\n",
                "    plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Key Takeaways\n",
                "\n",
                "### Advantages:\n",
                "- ✅ Simple and intuitive\n",
                "- ✅ No training phase (lazy learning)\n",
                "- ✅ Naturally handles non-linear relationships\n",
                "- ✅ No assumptions about data distribution\n",
                "- ✅ Can use distance weighting for better predictions\n",
                "\n",
                "### Disadvantages:\n",
                "- ❌ Slow prediction (must compute all distances)\n",
                "- ❌ Sensitive to feature scaling\n",
                "- ❌ Curse of dimensionality\n",
                "- ❌ Sensitive to outliers\n",
                "- ❌ Requires choosing k hyperparameter\n",
                "- ❌ Poor extrapolation beyond training data range\n",
                "\n",
                "### When to Use:\n",
                "- Small to medium-sized datasets\n",
                "- Low-dimensional feature space\n",
                "- Non-linear relationships\n",
                "- Need interpretable baseline model\n",
                "- Local patterns more important than global trends\n",
                "\n",
                "### Bias-Variance Trade-off:\n",
                "- **Small k**: Low bias, high variance (overfitting)\n",
                "- **Large k**: High bias, low variance (underfitting)\n",
                "- **Optimal k**: Balance between bias and variance"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}