{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AdaBoost Regressor From Scratch\n",
                "\n",
                "AdaBoost for regression (AdaBoost.R2) follows a similar principle to the classifier: it trains a sequence of weak learners where each subsequent learner focuses more on the samples that were difficult for the previous ones.\n",
                "\n",
                "## Key Concepts:\n",
                "- **Weighted Error**: Error relative to the current sample weights\n",
                "- **Beta (Confidence)**: Confidence in the learner based on its average error\n",
                "- **Weight Update**: Harder-to-predict samples increase in weight\n",
                "- **Final Prediction**: Weighted median of all learners"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import make_regression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from sklearn.ensemble import AdaBoostRegressor as SklearnAda"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. AdaBoost.R2 Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AdaBoostRegressor:\n",
                "    def __init__(self, n_estimators=50, max_depth=3):\n",
                "        self.n_estimators = n_estimators\n",
                "        self.max_depth = max_depth\n",
                "        self.estimators = []\n",
                "        self.betas = []\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        n_samples = X.shape[0]\n",
                "        w = np.full(n_samples, 1 / n_samples)\n",
                "        \n",
                "        self.estimators = []\n",
                "        self.betas = []\n",
                "\n",
                "        for _ in range(self.n_estimators):\n",
                "            # Train weak learner with weights\n",
                "            # Note: Sklearn's DecisionTreeRegressor supports sample_weight\n",
                "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
                "            tree.fit(X, y, sample_weight=w)\n",
                "            \n",
                "            y_pred = tree.predict(X)\n",
                "            error_abs = np.abs(y_pred - y)\n",
                "            max_error = np.max(error_abs)\n",
                "            if max_error == 0: break\n",
                "            \n",
                "            # Relative error\n",
                "            L = error_abs / max_error\n",
                "            avg_L = np.sum(w * L)\n",
                "            \n",
                "            if avg_L >= 0.5: break # Stop if weak learner is no better than random\n",
                "            \n",
                "            beta = avg_L / (1 - avg_L)\n",
                "            self.betas.append(beta)\n",
                "            self.estimators.append(tree)\n",
                "            \n",
                "            # Update weights\n",
                "            w *= (beta ** (1 - L))\n",
                "            w /= np.sum(w)\n",
                "\n",
                "    def predict(self, X):\n",
                "        # Weighted median prediction\n",
                "        # Calculate weights for each estimator: ln(1/beta)\n",
                "        weights = np.log(1 / np.array(self.betas))\n",
                "        predictions = np.array([tree.predict(X) for tree in self.estimators])\n",
                "        \n",
                "        final_preds = []\n",
                "        for i in range(X.shape[0]):\n",
                "            # Sort predictions and find weighted median\n",
                "            sample_preds = predictions[:, i]\n",
                "            sorted_idx = np.argsort(sample_preds)\n",
                "            sorted_preds = sample_preds[sorted_idx]\n",
                "            sorted_weights = weights[sorted_idx]\n",
                "            \n",
                "            cumulative_weight = np.cumsum(sorted_weights)\n",
                "            total_weight = np.sum(sorted_weights)\n",
                "            \n",
                "            median_idx = np.searchsorted(cumulative_weight, 0.5 * total_weight)\n",
                "            final_preds.append(sorted_preds[median_idx])\n",
                "            \n",
                "        return np.array(final_preds)\n",
                "\n",
                "    def score(self, X, y):\n",
                "        y_pred = self.predict(X)\n",
                "        return 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Testing and Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "ada = AdaBoostRegressor(n_estimators=50, max_depth=3)\n",
                "ada.fit(X_train, y_train)\n",
                "print(f\"Our AdaBoost R2: {ada.score(X_test, y_test):.4f}\")\n",
                "\n",
                "sk_ada = SklearnAda(n_estimators=50, random_state=42)\n",
                "sk_ada.fit(X_train, y_train)\n",
                "print(f\"Sklearn AdaBoost R2: {sk_ada.score(X_test, y_test):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Visualizing Fit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
                "y_line = ada.predict(X_line)\n",
                "\n",
                "plt.scatter(X, y, color='blue', alpha=0.5)\n",
                "plt.plot(X_line, y_line, color='red', linewidth=2)\n",
                "plt.title(\"AdaBoost Regression Line\")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}