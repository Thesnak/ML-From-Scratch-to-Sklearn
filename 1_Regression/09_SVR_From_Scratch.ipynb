{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Support Vector Regression (SVR) From Scratch\n",
                "\n",
                "SVR applies the principles of SVM to regression problems. It tries to find a function that deviates at most $\\epsilon$ from the actual targets for all training data, while being as flat as possible.\n",
                "\n",
                "## Key Concepts:\n",
                "- **Epsilon-insensitive Loss**: Errors less than $\\epsilon$ are ignored\n",
                "- **Tolerance ($\\epsilon$)**: The tube size around the regression line\n",
                "- **Regularization**: Penalizing large weights to ensure flatness\n",
                "- **Primal optimization**: Gradient descent on our structured loss function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import make_regression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.svm import SVR as SklearnSVR"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mathematical Foundation\n",
                "\n",
                "### Cost Function\n",
                "Minimize:\n",
                "$$J(w, b) = \\frac{1}{2}||w||^2 + C \\sum_{i=1}^n L_\\epsilon(y_i, f(x_i))$$\n",
                "\n",
                "Where $L_\\epsilon$ is the $\\epsilon$-insensitive loss function:\n",
                "$$L_\\epsilon(y, f(x)) = \\max(0, |y - f(x)| - \\epsilon)$$\n",
                "\n",
                "### Gradients (for linear SVR)\n",
                "If $|y_i - f(x_i)| \\leq \\epsilon$, the loss part is zero, only regularization contributes.\n",
                "If $|y_i - f(x_i)| > \\epsilon$, the gradient depends on the direction of error."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SVR:\n",
                "    def __init__(self, learning_rate=0.001, C=1.0, epsilon=0.1, n_iters=1000):\n",
                "        self.lr = learning_rate\n",
                "        self.C = C\n",
                "        self.epsilon = epsilon\n",
                "        self.n_iters = n_iters\n",
                "        self.w = None\n",
                "        self.b = None\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        n_samples, n_features = X.shape\n",
                "        self.w = np.zeros(n_features)\n",
                "        self.b = 0\n",
                "\n",
                "        for _ in range(self.n_iters):\n",
                "            for i in range(n_samples):\n",
                "                prediction = np.dot(X[i], self.w) + self.b\n",
                "                error = prediction - y[i]\n",
                "                \n",
                "                if abs(error) > self.epsilon:\n",
                "                    # Gradient for weight: w + C * sign(error) * x_i\n",
                "                    # Gradient for bias: C * sign(error)\n",
                "                    target_sign = np.sign(error)\n",
                "                    self.w -= self.lr * (self.w + self.C * target_sign * X[i])\n",
                "                    self.b -= self.lr * (self.C * target_sign)\n",
                "                else:\n",
                "                    self.w -= self.lr * self.w\n",
                "\n",
                "    def predict(self, X):\n",
                "        return np.dot(X, self.w) + self.b\n",
                "\n",
                "    def score(self, X, y):\n",
                "        y_pred = self.predict(X)\n",
                "        return 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Testing and Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X, y = make_regression(n_samples=100, n_features=1, noise=15, random_state=42)\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "svr = SVR(epsilon=10, C=10)\n",
                "svr.fit(X_train, y_train)\n",
                "print(f\"Our SVR R2: {svr.score(X_test, y_test):.4f}\")\n",
                "\n",
                "X_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
                "y_line = svr.predict(X_line)\n",
                "plt.scatter(X, y)\n",
                "plt.plot(X_line, y_line, color='red')\n",
                "plt.plot(X_line, y_line + svr.epsilon, color='grey', linestyle='--')\n",
                "plt.plot(X_line, y_line - svr.epsilon, color='grey', linestyle='--')\n",
                "plt.title(\"SVR with Epsilon Tube\")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}