{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gradient Boosting Classifier From Scratch\n",
                "\n",
                "Gradient Boosting for classification is slightly more involved than regression. Instead of predicting raw targets, we predict **log-odds** and use the **logistic loss** (log-loss).\n",
                "\n",
                "## Key Concepts:\n",
                "- **Log-Odds**: $L = \\log(\\frac{p}{1-p})$\n",
                "- **Probability**: $p = \\frac{1}{1 + e^{-L}}$\n",
                "- **Pseudo-Residuals**: $y_i - p_i$\n",
                "- **Gamma Calculation**: In each terminal node, we calculate the leaf value that minimizes the overall loss for classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from sklearn.datasets import make_moons\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Implementation\n",
                "\n",
                "We use `DecisionTreeRegressor` as the weak learner even for classification, but we adjust the leaf values (gamma) to minimize log-loss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GradientBoostingClassifier:\n",
                "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
                "        self.n_estimators = n_estimators\n",
                "        self.lr = learning_rate\n",
                "        self.max_depth = max_depth\n",
                "        self.trees = []\n",
                "        self.init_log_odds = None\n",
                "\n",
                "    def _sigmoid(self, x):\n",
                "        return 1 / (1 + np.exp(-x))\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        # Initial log-odds: log(y=1 / y=0)\n",
                "        p_mean = np.mean(y)\n",
                "        self.init_log_odds = np.log(p_mean / (1 - p_mean))\n",
                "        \n",
                "        current_log_odds = np.full(len(y), self.init_log_odds)\n",
                "        \n",
                "        for _ in range(self.n_estimators):\n",
                "            # 1. Calculate probabilities\n",
                "            probs = self._sigmoid(current_log_odds)\n",
                "            \n",
                "            # 2. Calculate residuals (y - p)\n",
                "            residuals = y - probs\n",
                "            \n",
                "            # 3. Fit tree to residuals\n",
                "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
                "            tree.fit(X, residuals)\n",
                "            \n",
                "            # 4. Update log-odds\n",
                "            # Simplification: in a proper GBM, we recalculate leaf values (Gamma)\n",
                "            # to minimize log-loss. For this educational implementation, \n",
                "            # we simply add the tree predictions scaled by the learning rate.\n",
                "            current_log_odds += self.lr * tree.predict(X)\n",
                "            self.trees.append(tree)\n",
                "\n",
                "    def predict_proba(self, X):\n",
                "        log_odds = np.full(X.shape[0], self.init_log_odds)\n",
                "        for tree in self.trees:\n",
                "            log_odds += self.lr * tree.predict(X)\n",
                "        return self._sigmoid(log_odds)\n",
                "\n",
                "    def predict(self, X):\n",
                "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
                "\n",
                "    def score(self, X, y):\n",
                "        return np.mean(self.predict(X) == y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Testing and Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "gbc = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, max_depth=2)\n",
                "gbc.fit(X_train, y_train)\n",
                "print(f\"Our GB Classifier Accuracy: {gbc.score(X_test, y_test):.4f}\")\n",
                "\n",
                "from sklearn.ensemble import GradientBoostingClassifier as SklearnGBC\n",
                "sk_gbc = SklearnGBC(n_estimators=50, learning_rate=0.1, max_depth=2, random_state=42)\n",
                "sk_gbc.fit(X_train, y_train)\n",
                "print(f\"Sklearn GB Classifier Accuracy: {sk_gbc.score(X_test, y_test):.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}