{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gradient Boosting Regression From Scratch\n",
                "\n",
                "Gradient Boosting is a powerful ensemble method that builds trees sequentially. Each new tree corrects the errors (residuals) made by the previous trees.\n",
                "\n",
                "## Key Concepts:\n",
                "- **Sequential Learning**: Trees are added one by one\n",
                "- **Residuals**: The current error ($y - y_{pred}$)\n",
                "- **Learning Rate (Shrinkage)**: Scaling factor for each tree's contribution\n",
                "- **Additive Model**: $F_m(x) = F_{m-1}(x) + \\eta h_m(x)$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from sklearn.datasets import make_regression\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Algorithm Overview\n",
                "\n",
                "1. Initialize prediction with mean: $F_0(x) = \\text{mean}(y)$\n",
                "2. For $m=1$ to $M$:\n",
                "   a. Calculate residuals: $r_{im} = y_i - F_{m-1}(x_i)$\n",
                "   b. Fit a weak learner $h_m(x)$ to the residuals $r_{im}$\n",
                "   c. Update the model: $F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GradientBoostingRegressor:\n",
                "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
                "        self.n_estimators = n_estimators\n",
                "        self.lr = learning_rate\n",
                "        self.max_depth = max_depth\n",
                "        self.trees = []\n",
                "        self.init_prediction = None\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        # Step 1: Initial prediction (mean of y)\n",
                "        self.init_prediction = np.mean(y)\n",
                "        current_predictions = np.full(len(y), self.init_prediction)\n",
                "        \n",
                "        for _ in range(self.n_estimators):\n",
                "            # Step 2a: Calculate residuals\n",
                "            residuals = y - current_predictions\n",
                "            \n",
                "            # Step 2b: Fit tree to residuals\n",
                "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
                "            tree.fit(X, residuals)\n",
                "            \n",
                "            # Step 2c: Update predictions\n",
                "            predictions = tree.predict(X)\n",
                "            current_predictions += self.lr * predictions\n",
                "            \n",
                "            self.trees.append(tree)\n",
                "\n",
                "    def predict(self, X):\n",
                "        # Initial prediction\n",
                "        y_pred = np.full(X.shape[0], self.init_prediction)\n",
                "        \n",
                "        # Add contributions from each tree\n",
                "        for tree in self.trees:\n",
                "            y_pred += self.lr * tree.predict(X)\n",
                "            \n",
                "        return y_pred\n",
                "\n",
                "    def score(self, X, y):\n",
                "        y_pred = self.predict(X)\n",
                "        return 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Testing and Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X, y = make_regression(n_samples=200, n_features=1, noise=20, random_state=42)\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=2)\n",
                "gbr.fit(X_train, y_train)\n",
                "print(f\"Our Gradient Boosting R2: {gbr.score(X_test, y_test):.4f}\")\n",
                "\n",
                "X_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
                "plt.scatter(X, y, alpha=0.5)\n",
                "plt.plot(X_line, gbr.predict(X_line), color='red', linewidth=3)\n",
                "plt.title(\"Gradient Boosting Fit\")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}