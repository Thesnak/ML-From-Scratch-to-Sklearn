{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Decision Tree Regression From Scratch\n",
                "\n",
                "Decision Tree Regression uses a tree structure to predict continuous values by recursively splitting the feature space.\n",
                "\n",
                "## Key Concepts:\n",
                "- **MSE Splitting Criterion**: Minimize mean squared error\n",
                "- **Variance Reduction**: Split to reduce variance in child nodes\n",
                "- **Leaf Predictions**: Mean of target values in leaf\n",
                "- **Non-parametric**: No assumptions about data distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import make_regression\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mathematical Foundation\n",
                "\n",
                "### Mean Squared Error (MSE):\n",
                "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\n",
                "\n",
                "### Variance Reduction:\n",
                "$$\\Delta = MSE_{parent} - \\left(\\frac{n_{left}}{n} MSE_{left} + \\frac{n_{right}}{n} MSE_{right}\\right)$$\n",
                "\n",
                "### Leaf Prediction:\n",
                "$$\\hat{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$$\n",
                "\n",
                "(Mean of all target values in the leaf)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Node:\n",
                "    def __init__(self, feature=None, threshold=None, value=None, left=None, right=None):\n",
                "        self.feature = feature      # Feature index to split on\n",
                "        self.threshold = threshold  # Threshold value for split\n",
                "        self.value = value         # Prediction value (for leaf nodes)\n",
                "        self.left = left           # Left child\n",
                "        self.right = right         # Right child\n",
                "\n",
                "class DecisionTreeRegressor:\n",
                "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
                "        \"\"\"\n",
                "        Initialize Decision Tree Regressor\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        max_depth : int or None\n",
                "            Maximum depth of the tree\n",
                "        min_samples_split : int\n",
                "            Minimum samples required to split a node\n",
                "        min_samples_leaf : int\n",
                "            Minimum samples required in a leaf node\n",
                "        \"\"\"\n",
                "        self.max_depth = max_depth\n",
                "        self.min_samples_split = min_samples_split\n",
                "        self.min_samples_leaf = min_samples_leaf\n",
                "        self.root = None\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        \"\"\"\n",
                "        Fit the decision tree\n",
                "        \"\"\"\n",
                "        self.root = self._build_tree(X, y, depth=0)\n",
                "        return self\n",
                "    \n",
                "    def _mse(self, y):\n",
                "        \"\"\"\n",
                "        Calculate mean squared error\n",
                "        \"\"\"\n",
                "        if len(y) == 0:\n",
                "            return 0\n",
                "        return np.var(y) * len(y)  # Variance * n = sum of squared deviations\n",
                "    \n",
                "    def _variance_reduction(self, y, y_left, y_right):\n",
                "        \"\"\"\n",
                "        Calculate variance reduction from a split\n",
                "        \"\"\"\n",
                "        n = len(y)\n",
                "        n_left, n_right = len(y_left), len(y_right)\n",
                "        \n",
                "        if n_left == 0 or n_right == 0:\n",
                "            return 0\n",
                "        \n",
                "        parent_mse = self._mse(y)\n",
                "        child_mse = (n_left / n) * self._mse(y_left) + (n_right / n) * self._mse(y_right)\n",
                "        \n",
                "        return parent_mse - child_mse\n",
                "    \n",
                "    def _best_split(self, X, y):\n",
                "        \"\"\"\n",
                "        Find the best split for the data\n",
                "        \"\"\"\n",
                "        best_gain = -1\n",
                "        best_feature = None\n",
                "        best_threshold = None\n",
                "        \n",
                "        n_features = X.shape[1]\n",
                "        \n",
                "        for feature in range(n_features):\n",
                "            thresholds = np.unique(X[:, feature])\n",
                "            \n",
                "            for threshold in thresholds:\n",
                "                # Split data\n",
                "                left_mask = X[:, feature] <= threshold\n",
                "                right_mask = ~left_mask\n",
                "                \n",
                "                # Check minimum samples constraint\n",
                "                if np.sum(left_mask) < self.min_samples_leaf or np.sum(right_mask) < self.min_samples_leaf:\n",
                "                    continue\n",
                "                \n",
                "                y_left, y_right = y[left_mask], y[right_mask]\n",
                "                \n",
                "                # Calculate variance reduction\n",
                "                gain = self._variance_reduction(y, y_left, y_right)\n",
                "                \n",
                "                if gain > best_gain:\n",
                "                    best_gain = gain\n",
                "                    best_feature = feature\n",
                "                    best_threshold = threshold\n",
                "        \n",
                "        return best_feature, best_threshold, best_gain\n",
                "    \n",
                "    def _build_tree(self, X, y, depth):\n",
                "        \"\"\"\n",
                "        Recursively build the decision tree\n",
                "        \"\"\"\n",
                "        n_samples = len(y)\n",
                "        \n",
                "        # Stopping criteria\n",
                "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
                "           n_samples < self.min_samples_split:\n",
                "            return Node(value=np.mean(y))\n",
                "        \n",
                "        # Find best split\n",
                "        best_feature, best_threshold, best_gain = self._best_split(X, y)\n",
                "        \n",
                "        # If no good split found, create leaf\n",
                "        if best_gain == 0 or best_feature is None:\n",
                "            return Node(value=np.mean(y))\n",
                "        \n",
                "        # Split data\n",
                "        left_mask = X[:, best_feature] <= best_threshold\n",
                "        right_mask = ~left_mask\n",
                "        \n",
                "        # Recursively build left and right subtrees\n",
                "        left_child = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
                "        right_child = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
                "        \n",
                "        return Node(feature=best_feature, threshold=best_threshold, \n",
                "                   left=left_child, right=right_child)\n",
                "    \n",
                "    def _predict_single(self, x, node):\n",
                "        \"\"\"\n",
                "        Predict value for a single sample\n",
                "        \"\"\"\n",
                "        # If leaf node, return value\n",
                "        if node.value is not None:\n",
                "            return node.value\n",
                "        \n",
                "        # Traverse tree\n",
                "        if x[node.feature] <= node.threshold:\n",
                "            return self._predict_single(x, node.left)\n",
                "        else:\n",
                "            return self._predict_single(x, node.right)\n",
                "    \n",
                "    def predict(self, X):\n",
                "        \"\"\"\n",
                "        Predict values for multiple samples\n",
                "        \"\"\"\n",
                "        return np.array([self._predict_single(x, self.root) for x in X])\n",
                "    \n",
                "    def score(self, X, y):\n",
                "        \"\"\"\n",
                "        Calculate R² score\n",
                "        \"\"\"\n",
                "        y_pred = self.predict(X)\n",
                "        ss_res = np.sum((y - y_pred) ** 2)\n",
                "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
                "        return 1 - (ss_res / ss_tot)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Testing on Synthetic Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate synthetic data\n",
                "np.random.seed(42)\n",
                "X, y = make_regression(n_samples=200, n_features=1, noise=20, random_state=42)\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "print(f\"Training samples: {X_train.shape[0]}\")\n",
                "print(f\"Test samples: {X_test.shape[0]}\")\n",
                "print(f\"Features: {X_train.shape[1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Decision Tree Regressor\n",
                "dt = DecisionTreeRegressor(max_depth=5)\n",
                "dt.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred_train = dt.predict(X_train)\n",
                "y_pred_test = dt.predict(X_test)\n",
                "\n",
                "# Calculate scores\n",
                "train_score = dt.score(X_train, y_train)\n",
                "test_score = dt.score(X_test, y_test)\n",
                "\n",
                "print(f\"\\nDecision Tree Regressor (max_depth={dt.max_depth})\")\n",
                "print(f\"Train R² Score: {train_score:.4f}\")\n",
                "print(f\"Test R² Score: {test_score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Comparison with Scikit-learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.tree import DecisionTreeRegressor as SklearnDTR\n",
                "\n",
                "# Train sklearn Decision Tree\n",
                "sklearn_dt = SklearnDTR(max_depth=5, random_state=42)\n",
                "sklearn_dt.fit(X_train, y_train)\n",
                "\n",
                "# Compare scores\n",
                "sklearn_train_score = sklearn_dt.score(X_train, y_train)\n",
                "sklearn_test_score = sklearn_dt.score(X_test, y_test)\n",
                "\n",
                "print(\"\\nComparison:\")\n",
                "print(f\"{'Method':<25} {'Train R²':<12} {'Test R²':<12}\")\n",
                "print(\"-\" * 49)\n",
                "print(f\"{'Our Decision Tree':<25} {train_score:<12.4f} {test_score:<12.4f}\")\n",
                "print(f\"{'Sklearn Decision Tree':<25} {sklearn_train_score:<12.4f} {sklearn_test_score:<12.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Effect of Max Depth"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different max_depth values\n",
                "depths = range(1, 16)\n",
                "train_scores = []\n",
                "test_scores = []\n",
                "\n",
                "for depth in depths:\n",
                "    dt = DecisionTreeRegressor(max_depth=depth)\n",
                "    dt.fit(X_train, y_train)\n",
                "    \n",
                "    train_scores.append(dt.score(X_train, y_train))\n",
                "    test_scores.append(dt.score(X_test, y_test))\n",
                "\n",
                "# Plot results\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(depths, train_scores, 'o-', label='Train R²', linewidth=2)\n",
                "plt.plot(depths, test_scores, 's-', label='Test R²', linewidth=2)\n",
                "plt.xlabel('Max Depth', fontsize=12)\n",
                "plt.ylabel('R² Score', fontsize=12)\n",
                "plt.title('Decision Tree Regression: Effect of Max Depth', fontsize=14)\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "# Find best depth\n",
                "best_depth = depths[np.argmax(test_scores)]\n",
                "best_score = max(test_scores)\n",
                "print(f\"\\nBest Max Depth: {best_depth}\")\n",
                "print(f\"Best Test R²: {best_score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualization of Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train trees with different depths\n",
                "dt_shallow = DecisionTreeRegressor(max_depth=2)\n",
                "dt_medium = DecisionTreeRegressor(max_depth=5)\n",
                "dt_deep = DecisionTreeRegressor(max_depth=10)\n",
                "\n",
                "dt_shallow.fit(X_train, y_train)\n",
                "dt_medium.fit(X_train, y_train)\n",
                "dt_deep.fit(X_train, y_train)\n",
                "\n",
                "# Create smooth line for predictions\n",
                "X_line = np.linspace(X_train.min(), X_train.max(), 300).reshape(-1, 1)\n",
                "y_pred_shallow = dt_shallow.predict(X_line)\n",
                "y_pred_medium = dt_medium.predict(X_line)\n",
                "y_pred_deep = dt_deep.predict(X_line)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(15, 5))\n",
                "\n",
                "for i, (depth, y_pred, title) in enumerate([\n",
                "    (2, y_pred_shallow, 'Max Depth=2 (Underfitting)'),\n",
                "    (5, y_pred_medium, 'Max Depth=5 (Good Fit)'),\n",
                "    (10, y_pred_deep, 'Max Depth=10 (Overfitting)')\n",
                "]):\n",
                "    plt.subplot(1, 3, i+1)\n",
                "    plt.scatter(X_train, y_train, alpha=0.5, s=30, label='Training Data')\n",
                "    plt.plot(X_line, y_pred, 'r-', linewidth=2, label=f'Decision Tree')\n",
                "    plt.xlabel('Feature', fontsize=11)\n",
                "    plt.ylabel('Target', fontsize=11)\n",
                "    plt.title(title, fontsize=12)\n",
                "    plt.legend(fontsize=10)\n",
                "    plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Key Takeaways\n",
                "\n",
                "### Advantages:\n",
                "- ✅ Handles non-linear relationships\n",
                "- ✅ No feature scaling required\n",
                "- ✅ Interpretable (can visualize tree)\n",
                "- ✅ Handles missing values (with modifications)\n",
                "- ✅ Feature importance available\n",
                "- ✅ No assumptions about data distribution\n",
                "\n",
                "### Disadvantages:\n",
                "- ❌ Prone to overfitting (especially deep trees)\n",
                "- ❌ High variance (small data changes → different tree)\n",
                "- ❌ Biased toward features with more levels\n",
                "- ❌ Not smooth predictions (piecewise constant)\n",
                "- ❌ Poor extrapolation\n",
                "\n",
                "### When to Use:\n",
                "- Non-linear relationships\n",
                "- Need interpretability\n",
                "- Mixed feature types\n",
                "- As base learner for ensemble methods\n",
                "- Baseline model\n",
                "\n",
                "### Hyperparameter Tuning:\n",
                "- **max_depth**: Controls overfitting (lower = less overfitting)\n",
                "- **min_samples_split**: Minimum samples to split (higher = less overfitting)\n",
                "- **min_samples_leaf**: Minimum samples in leaf (higher = smoother)\n",
                "\n",
                "### Comparison with Linear Regression:\n",
                "- **Decision Tree**: Better for non-linear data, prone to overfitting\n",
                "- **Linear Regression**: Better for linear data, more stable predictions"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}