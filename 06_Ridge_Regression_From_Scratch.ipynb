{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ridge Regression From Scratch\n",
                "\n",
                "Ridge Regression is a regularized version of Linear Regression that adds an **L2 penalty** to prevent overfitting.\n",
                "\n",
                "## Key Concepts:\n",
                "- **L2 Regularization**: Adds penalty proportional to the square of coefficients\n",
                "- **Alpha (λ)**: Regularization strength parameter\n",
                "- **Prevents Overfitting**: Shrinks coefficients toward zero\n",
                "- **Handles Multicollinearity**: Works well when features are correlated"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import make_regression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mathematical Foundation\n",
                "\n",
                "### Linear Regression Cost Function:\n",
                "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
                "\n",
                "### Ridge Regression Cost Function (with L2 penalty):\n",
                "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\alpha \\sum_{j=1}^{n} \\theta_j^2$$\n",
                "\n",
                "### Closed-Form Solution:\n",
                "$$\\theta = (X^T X + \\alpha I)^{-1} X^T y$$\n",
                "\n",
                "where:\n",
                "- $\\alpha$ is the regularization parameter\n",
                "- $I$ is the identity matrix"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RidgeRegression:\n",
                "    def __init__(self, alpha=1.0):\n",
                "        \"\"\"\n",
                "        Initialize Ridge Regression\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        alpha : float\n",
                "            Regularization strength (default=1.0)\n",
                "            - alpha = 0: equivalent to Linear Regression\n",
                "            - alpha > 0: stronger regularization\n",
                "        \"\"\"\n",
                "        self.alpha = alpha\n",
                "        self.weights = None\n",
                "        self.bias = None\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        \"\"\"\n",
                "        Fit Ridge Regression using closed-form solution\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        X : array-like, shape (n_samples, n_features)\n",
                "        y : array-like, shape (n_samples,)\n",
                "        \"\"\"\n",
                "        # Add bias term (column of ones)\n",
                "        n_samples, n_features = X.shape\n",
                "        X_with_bias = np.c_[np.ones((n_samples, 1)), X]\n",
                "        \n",
                "        # Create identity matrix (don't penalize bias term)\n",
                "        I = np.eye(n_features + 1)\n",
                "        I[0, 0] = 0  # Don't regularize bias\n",
                "        \n",
                "        # Closed-form solution: θ = (X^T X + αI)^(-1) X^T y\n",
                "        XtX = X_with_bias.T @ X_with_bias\n",
                "        theta = np.linalg.inv(XtX + self.alpha * I) @ X_with_bias.T @ y\n",
                "        \n",
                "        # Extract bias and weights\n",
                "        self.bias = theta[0]\n",
                "        self.weights = theta[1:]\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def predict(self, X):\n",
                "        \"\"\"\n",
                "        Make predictions\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        X : array-like, shape (n_samples, n_features)\n",
                "        \n",
                "        Returns:\n",
                "        --------\n",
                "        y_pred : array, shape (n_samples,)\n",
                "        \"\"\"\n",
                "        return X @ self.weights + self.bias\n",
                "    \n",
                "    def score(self, X, y):\n",
                "        \"\"\"\n",
                "        Calculate R² score\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        X : array-like, shape (n_samples, n_features)\n",
                "        y : array-like, shape (n_samples,)\n",
                "        \n",
                "        Returns:\n",
                "        --------\n",
                "        r2_score : float\n",
                "        \"\"\"\n",
                "        y_pred = self.predict(X)\n",
                "        ss_res = np.sum((y - y_pred) ** 2)\n",
                "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
                "        return 1 - (ss_res / ss_tot)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Testing on Synthetic Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate synthetic data with noise\n",
                "np.random.seed(42)\n",
                "X, y = make_regression(n_samples=100, n_features=10, noise=20, random_state=42)\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Standardize features (important for regularization)\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"Training samples: {X_train.shape[0]}\")\n",
                "print(f\"Test samples: {X_test.shape[0]}\")\n",
                "print(f\"Features: {X_train.shape[1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Ridge Regression\n",
                "ridge = RidgeRegression(alpha=1.0)\n",
                "ridge.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred_train = ridge.predict(X_train_scaled)\n",
                "y_pred_test = ridge.predict(X_test_scaled)\n",
                "\n",
                "# Calculate scores\n",
                "train_score = ridge.score(X_train_scaled, y_train)\n",
                "test_score = ridge.score(X_test_scaled, y_test)\n",
                "\n",
                "print(f\"\\nRidge Regression (alpha={ridge.alpha})\")\n",
                "print(f\"Train R² Score: {train_score:.4f}\")\n",
                "print(f\"Test R² Score: {test_score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Comparison with Scikit-learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import Ridge as SklearnRidge\n",
                "\n",
                "# Train sklearn Ridge\n",
                "sklearn_ridge = SklearnRidge(alpha=1.0)\n",
                "sklearn_ridge.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Compare scores\n",
                "sklearn_train_score = sklearn_ridge.score(X_train_scaled, y_train)\n",
                "sklearn_test_score = sklearn_ridge.score(X_test_scaled, y_test)\n",
                "\n",
                "print(\"\\nComparison:\")\n",
                "print(f\"{'Method':<20} {'Train R²':<12} {'Test R²':<12}\")\n",
                "print(\"-\" * 44)\n",
                "print(f\"{'Our Ridge':<20} {train_score:<12.4f} {test_score:<12.4f}\")\n",
                "print(f\"{'Sklearn Ridge':<20} {sklearn_train_score:<12.4f} {sklearn_test_score:<12.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Effect of Alpha (Regularization Strength)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different alpha values\n",
                "alphas = [0, 0.1, 1, 10, 100, 1000]\n",
                "train_scores = []\n",
                "test_scores = []\n",
                "\n",
                "for alpha in alphas:\n",
                "    ridge = RidgeRegression(alpha=alpha)\n",
                "    ridge.fit(X_train_scaled, y_train)\n",
                "    \n",
                "    train_scores.append(ridge.score(X_train_scaled, y_train))\n",
                "    test_scores.append(ridge.score(X_test_scaled, y_test))\n",
                "\n",
                "# Plot results\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.semilogx(alphas, train_scores, 'o-', label='Train Score', linewidth=2)\n",
                "plt.semilogx(alphas, test_scores, 's-', label='Test Score', linewidth=2)\n",
                "plt.xlabel('Alpha (Regularization Strength)', fontsize=12)\n",
                "plt.ylabel('R² Score', fontsize=12)\n",
                "plt.title('Ridge Regression: Effect of Regularization', fontsize=14)\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nAlpha vs Performance:\")\n",
                "print(f\"{'Alpha':<10} {'Train R²':<12} {'Test R²':<12}\")\n",
                "print(\"-\" * 34)\n",
                "for i, alpha in enumerate(alphas):\n",
                "    print(f\"{alpha:<10} {train_scores[i]:<12.4f} {test_scores[i]:<12.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Coefficient Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare coefficients with different alphas\n",
                "ridge_weak = RidgeRegression(alpha=0.1)\n",
                "ridge_strong = RidgeRegression(alpha=100)\n",
                "\n",
                "ridge_weak.fit(X_train_scaled, y_train)\n",
                "ridge_strong.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Plot coefficient magnitudes\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "x_pos = np.arange(len(ridge_weak.weights))\n",
                "width = 0.35\n",
                "\n",
                "plt.bar(x_pos - width/2, ridge_weak.weights, width, label='Alpha=0.1 (Weak)', alpha=0.8)\n",
                "plt.bar(x_pos + width/2, ridge_strong.weights, width, label='Alpha=100 (Strong)', alpha=0.8)\n",
                "\n",
                "plt.xlabel('Feature Index', fontsize=12)\n",
                "plt.ylabel('Coefficient Value', fontsize=12)\n",
                "plt.title('Ridge Regression: Coefficient Shrinkage', fontsize=14)\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3, axis='y')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nCoefficient L2 Norms:\")\n",
                "print(f\"Alpha=0.1:  {np.linalg.norm(ridge_weak.weights):.4f}\")\n",
                "print(f\"Alpha=100:  {np.linalg.norm(ridge_strong.weights):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Key Takeaways\n",
                "\n",
                "### Advantages:\n",
                "- ✅ Prevents overfitting through regularization\n",
                "- ✅ Handles multicollinearity well\n",
                "- ✅ Has closed-form solution (fast training)\n",
                "- ✅ All features retained (unlike Lasso)\n",
                "\n",
                "### Disadvantages:\n",
                "- ❌ Doesn't perform feature selection\n",
                "- ❌ Requires feature scaling\n",
                "- ❌ Need to tune alpha hyperparameter\n",
                "\n",
                "### When to Use:\n",
                "- Many correlated features\n",
                "- Want to keep all features\n",
                "- Prevent overfitting on high-dimensional data\n",
                "- Linear relationships in data"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}