{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Naive Bayes From Scratch\n",
                "\n",
                "Naive Bayes is a probabilistic classifier based on Bayes' theorem with the \"naive\" assumption of conditional independence between features.\n",
                "\n",
                "## Key Concepts:\n",
                "- **Bayes' Theorem**: P(y|X) = P(X|y) * P(y) / P(X)\n",
                "- **Naive Assumption**: Features are conditionally independent given the class\n",
                "- **Three Variants**: Gaussian, Multinomial, Bernoulli\n",
                "- **Fast Training**: Simple probability calculations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import load_iris, fetch_20newsgroups\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import CountVectorizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mathematical Foundation\n",
                "\n",
                "### Bayes' Theorem:\n",
                "$$P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}$$\n",
                "\n",
                "### Naive Bayes Assumption:\n",
                "$$P(X|y) = P(x_1|y) \\cdot P(x_2|y) \\cdot ... \\cdot P(x_n|y)$$\n",
                "\n",
                "### Classification Rule:\n",
                "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i|y)$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Gaussian Naive Bayes\n",
                "\n",
                "For continuous features, assumes Gaussian (normal) distribution:\n",
                "$$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2}\\right)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GaussianNB:\n",
                "    def __init__(self):\n",
                "        self.classes = None\n",
                "        self.class_priors = {}\n",
                "        self.means = {}\n",
                "        self.variances = {}\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        \"\"\"\n",
                "        Fit Gaussian Naive Bayes\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        X : array-like, shape (n_samples, n_features)\n",
                "        y : array-like, shape (n_samples,)\n",
                "        \"\"\"\n",
                "        self.classes = np.unique(y)\n",
                "        n_samples = len(y)\n",
                "        \n",
                "        # Calculate prior probabilities and parameters for each class\n",
                "        for c in self.classes:\n",
                "            X_c = X[y == c]\n",
                "            \n",
                "            # Prior probability: P(y=c)\n",
                "            self.class_priors[c] = len(X_c) / n_samples\n",
                "            \n",
                "            # Mean and variance for each feature\n",
                "            self.means[c] = np.mean(X_c, axis=0)\n",
                "            self.variances[c] = np.var(X_c, axis=0) + 1e-9  # Add small value to avoid division by zero\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def _gaussian_pdf(self, x, mean, var):\n",
                "        \"\"\"\n",
                "        Calculate Gaussian probability density function\n",
                "        \"\"\"\n",
                "        return (1 / np.sqrt(2 * np.pi * var)) * np.exp(-((x - mean) ** 2) / (2 * var))\n",
                "    \n",
                "    def _predict_single(self, x):\n",
                "        \"\"\"\n",
                "        Predict class for a single sample\n",
                "        \"\"\"\n",
                "        posteriors = []\n",
                "        \n",
                "        for c in self.classes:\n",
                "            # Start with prior probability (in log space to avoid underflow)\n",
                "            posterior = np.log(self.class_priors[c])\n",
                "            \n",
                "            # Add log likelihood for each feature\n",
                "            for i in range(len(x)):\n",
                "                likelihood = self._gaussian_pdf(x[i], self.means[c][i], self.variances[c][i])\n",
                "                posterior += np.log(likelihood + 1e-10)  # Avoid log(0)\n",
                "            \n",
                "            posteriors.append(posterior)\n",
                "        \n",
                "        # Return class with highest posterior probability\n",
                "        return self.classes[np.argmax(posteriors)]\n",
                "    \n",
                "    def predict(self, X):\n",
                "        \"\"\"\n",
                "        Predict classes for multiple samples\n",
                "        \"\"\"\n",
                "        return np.array([self._predict_single(x) for x in X])\n",
                "    \n",
                "    def score(self, X, y):\n",
                "        \"\"\"\n",
                "        Calculate accuracy\n",
                "        \"\"\"\n",
                "        return np.mean(self.predict(X) == y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Testing Gaussian NB on Iris Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Iris dataset\n",
                "iris = load_iris()\n",
                "X, y = iris.data, iris.target\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                "\n",
                "# Train Gaussian NB\n",
                "gnb = GaussianNB()\n",
                "gnb.fit(X_train, y_train)\n",
                "\n",
                "# Evaluate\n",
                "train_acc = gnb.score(X_train, y_train)\n",
                "test_acc = gnb.score(X_test, y_test)\n",
                "\n",
                "print(\"Gaussian Naive Bayes on Iris Dataset\")\n",
                "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
                "print(f\"Test Accuracy: {test_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Multinomial Naive Bayes\n",
                "\n",
                "For discrete count features (e.g., word counts in text):\n",
                "$$P(x_i|y) = \\frac{N_{yi} + \\alpha}{N_y + \\alpha n}$$\n",
                "\n",
                "where $N_{yi}$ is count of feature $i$ in class $y$, and $\\alpha$ is smoothing parameter."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultinomialNB:\n",
                "    def __init__(self, alpha=1.0):\n",
                "        \"\"\"\n",
                "        Initialize Multinomial Naive Bayes\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        alpha : float\n",
                "            Additive (Laplace/Lidstone) smoothing parameter (default=1.0)\n",
                "        \"\"\"\n",
                "        self.alpha = alpha\n",
                "        self.classes = None\n",
                "        self.class_priors = {}\n",
                "        self.feature_probs = {}\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        \"\"\"\n",
                "        Fit Multinomial Naive Bayes\n",
                "        \"\"\"\n",
                "        self.classes = np.unique(y)\n",
                "        n_samples, n_features = X.shape\n",
                "        \n",
                "        for c in self.classes:\n",
                "            X_c = X[y == c]\n",
                "            \n",
                "            # Prior probability\n",
                "            self.class_priors[c] = len(X_c) / n_samples\n",
                "            \n",
                "            # Feature probabilities with Laplace smoothing\n",
                "            feature_counts = np.sum(X_c, axis=0)\n",
                "            total_count = np.sum(feature_counts)\n",
                "            self.feature_probs[c] = (feature_counts + self.alpha) / (total_count + self.alpha * n_features)\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def _predict_single(self, x):\n",
                "        \"\"\"\n",
                "        Predict class for a single sample\n",
                "        \"\"\"\n",
                "        posteriors = []\n",
                "        \n",
                "        for c in self.classes:\n",
                "            # Log prior\n",
                "            posterior = np.log(self.class_priors[c])\n",
                "            \n",
                "            # Log likelihood\n",
                "            posterior += np.sum(x * np.log(self.feature_probs[c] + 1e-10))\n",
                "            \n",
                "            posteriors.append(posterior)\n",
                "        \n",
                "        return self.classes[np.argmax(posteriors)]\n",
                "    \n",
                "    def predict(self, X):\n",
                "        return np.array([self._predict_single(x) for x in X])\n",
                "    \n",
                "    def score(self, X, y):\n",
                "        return np.mean(self.predict(X) == y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Bernoulli Naive Bayes\n",
                "\n",
                "For binary features:\n",
                "$$P(x_i|y) = P(i|y)x_i + (1 - P(i|y))(1 - x_i)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BernoulliNB:\n",
                "    def __init__(self, alpha=1.0):\n",
                "        \"\"\"\n",
                "        Initialize Bernoulli Naive Bayes\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        alpha : float\n",
                "            Additive smoothing parameter (default=1.0)\n",
                "        \"\"\"\n",
                "        self.alpha = alpha\n",
                "        self.classes = None\n",
                "        self.class_priors = {}\n",
                "        self.feature_probs = {}\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        \"\"\"\n",
                "        Fit Bernoulli Naive Bayes\n",
                "        \"\"\"\n",
                "        self.classes = np.unique(y)\n",
                "        n_samples, n_features = X.shape\n",
                "        \n",
                "        for c in self.classes:\n",
                "            X_c = X[y == c]\n",
                "            n_c = len(X_c)\n",
                "            \n",
                "            # Prior probability\n",
                "            self.class_priors[c] = n_c / n_samples\n",
                "            \n",
                "            # Feature probabilities with smoothing\n",
                "            # P(feature=1|class)\n",
                "            self.feature_probs[c] = (np.sum(X_c, axis=0) + self.alpha) / (n_c + 2 * self.alpha)\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def _predict_single(self, x):\n",
                "        \"\"\"\n",
                "        Predict class for a single sample\n",
                "        \"\"\"\n",
                "        posteriors = []\n",
                "        \n",
                "        for c in self.classes:\n",
                "            # Log prior\n",
                "            posterior = np.log(self.class_priors[c])\n",
                "            \n",
                "            # Log likelihood\n",
                "            for i in range(len(x)):\n",
                "                p = self.feature_probs[c][i]\n",
                "                if x[i] == 1:\n",
                "                    posterior += np.log(p + 1e-10)\n",
                "                else:\n",
                "                    posterior += np.log(1 - p + 1e-10)\n",
                "            \n",
                "            posteriors.append(posterior)\n",
                "        \n",
                "        return self.classes[np.argmax(posteriors)]\n",
                "    \n",
                "    def predict(self, X):\n",
                "        return np.array([self._predict_single(x) for x in X])\n",
                "    \n",
                "    def score(self, X, y):\n",
                "        return np.mean(self.predict(X) == y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Comparison with Scikit-learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.naive_bayes import GaussianNB as SklearnGaussianNB\n",
                "\n",
                "# Train sklearn Gaussian NB\n",
                "sklearn_gnb = SklearnGaussianNB()\n",
                "sklearn_gnb.fit(X_train, y_train)\n",
                "\n",
                "sklearn_train_acc = sklearn_gnb.score(X_train, y_train)\n",
                "sklearn_test_acc = sklearn_gnb.score(X_test, y_test)\n",
                "\n",
                "print(\"\\nGaussian NB Comparison:\")\n",
                "print(f\"{'Method':<20} {'Train Acc':<12} {'Test Acc':<12}\")\n",
                "print(\"-\" * 44)\n",
                "print(f\"{'Our Gaussian NB':<20} {train_acc:<12.4f} {test_acc:<12.4f}\")\n",
                "print(f\"{'Sklearn Gaussian NB':<20} {sklearn_train_acc:<12.4f} {sklearn_test_acc:<12.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Testing on Binary Data (Bernoulli NB)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create binary dataset\n",
                "np.random.seed(42)\n",
                "X_binary = np.random.randint(0, 2, size=(200, 10))\n",
                "y_binary = (np.sum(X_binary, axis=1) > 5).astype(int)\n",
                "\n",
                "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n",
                "    X_binary, y_binary, test_size=0.3, random_state=42\n",
                ")\n",
                "\n",
                "# Train Bernoulli NB\n",
                "bnb = BernoulliNB()\n",
                "bnb.fit(X_train_bin, y_train_bin)\n",
                "\n",
                "bnb_train_acc = bnb.score(X_train_bin, y_train_bin)\n",
                "bnb_test_acc = bnb.score(X_test_bin, y_test_bin)\n",
                "\n",
                "print(\"\\nBernoulli Naive Bayes on Binary Data\")\n",
                "print(f\"Train Accuracy: {bnb_train_acc:.4f}\")\n",
                "print(f\"Test Accuracy: {bnb_test_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Visualization: Feature Distributions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize learned Gaussian distributions for first feature\n",
                "feature_idx = 0\n",
                "x_range = np.linspace(X[:, feature_idx].min(), X[:, feature_idx].max(), 100)\n",
                "\n",
                "plt.figure(figsize=(12, 4))\n",
                "\n",
                "for i, c in enumerate(gnb.classes):\n",
                "    plt.subplot(1, 3, i+1)\n",
                "    \n",
                "    # Plot histogram of actual data\n",
                "    plt.hist(X[y == c, feature_idx], bins=15, density=True, alpha=0.5, label='Actual')\n",
                "    \n",
                "    # Plot learned Gaussian distribution\n",
                "    mean = gnb.means[c][feature_idx]\n",
                "    var = gnb.variances[c][feature_idx]\n",
                "    pdf = gnb._gaussian_pdf(x_range, mean, var)\n",
                "    plt.plot(x_range, pdf, 'r-', linewidth=2, label='Learned Gaussian')\n",
                "    \n",
                "    plt.xlabel(iris.feature_names[feature_idx], fontsize=11)\n",
                "    plt.ylabel('Density', fontsize=11)\n",
                "    plt.title(f'Class: {iris.target_names[c]}', fontsize=12)\n",
                "    plt.legend(fontsize=10)\n",
                "    plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Key Takeaways\n",
                "\n",
                "### Advantages:\n",
                "- ✅ Fast training and prediction\n",
                "- ✅ Works well with small datasets\n",
                "- ✅ Handles high-dimensional data well\n",
                "- ✅ Naturally handles multi-class problems\n",
                "- ✅ Probabilistic predictions\n",
                "- ✅ Simple and interpretable\n",
                "\n",
                "### Disadvantages:\n",
                "- ❌ Naive independence assumption (rarely true)\n",
                "- ❌ Can be outperformed by more complex models\n",
                "- ❌ Sensitive to irrelevant features\n",
                "- ❌ Zero-frequency problem (solved by smoothing)\n",
                "\n",
                "### When to Use Each Variant:\n",
                "\n",
                "**Gaussian NB:**\n",
                "- Continuous features\n",
                "- Features follow normal distribution\n",
                "- Examples: Iris classification, sensor data\n",
                "\n",
                "**Multinomial NB:**\n",
                "- Discrete count features\n",
                "- Text classification (word counts)\n",
                "- Examples: Document classification, spam detection\n",
                "\n",
                "**Bernoulli NB:**\n",
                "- Binary features\n",
                "- Text classification (word presence/absence)\n",
                "- Examples: Sentiment analysis, binary feature data\n",
                "\n",
                "### Best Practices:\n",
                "- Use smoothing to handle zero probabilities\n",
                "- Feature scaling not required (unlike many algorithms)\n",
                "- Works well as a baseline model\n",
                "- Excellent for text classification tasks"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}