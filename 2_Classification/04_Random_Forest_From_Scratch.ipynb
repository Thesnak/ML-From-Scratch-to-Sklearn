{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Random Forest From Scratch\n",
                "\n",
                "In this notebook, we will implement the **Random Forest** algorithm from scratch. We will reuse the `DecisionTree` implementation from our previous work and extend it with bootstrapping and aggregation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from collections import Counter\n",
                "import math\n",
                "import random"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Decision Tree Implementation (Reused)\n",
                "\n",
                "First, we include the necessary components for a single Decision Tree: `entropy`, `information_gain`, `Node`, and `DecisionTree`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def entropy(y):\n",
                "    counts = Counter(y)\n",
                "    total = len(y)\n",
                "    ent = 0\n",
                "    for count in counts.values():\n",
                "        p = count / total\n",
                "        ent -= p * math.log2(p)\n",
                "    return ent\n",
                "\n",
                "def information_gain(X, y, feature_index):\n",
                "    total_entropy = entropy(y)\n",
                "    values = set(row[feature_index] for row in X)\n",
                "    weighted_entropy = 0\n",
                "    \n",
                "    for value in values:\n",
                "        sub_y = [\n",
                "            y[i] for i in range(len(y))\n",
                "            if X[i][feature_index] == value\n",
                "        ]\n",
                "        weighted_entropy += (len(sub_y) / len(y)) * entropy(sub_y)\n",
                "        \n",
                "    return total_entropy - weighted_entropy\n",
                "\n",
                "class Node:\n",
                "    def __init__(self, feature=None, value=None, label=None):\n",
                "        self.feature = feature    # feature index\n",
                "        self.value = value        # feature value for split (categorical here)\n",
                "        self.label = label        # class label (for leaf)\n",
                "        self.children = {}\n",
                "\n",
                "class DecisionTree:\n",
                "    def __init__(self, max_depth=None, feature_subset_size=None):\n",
                "        self.max_depth = max_depth\n",
                "        self.feature_subset_size = feature_subset_size # For Random Forest\n",
                "        self.root = None\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        self.root = self._build_tree(X, y, depth=0)\n",
                "\n",
                "    def _build_tree(self, X, y, depth):\n",
                "        # 1. Base cases\n",
                "        if len(set(y)) == 1:\n",
                "            return Node(label=y[0])\n",
                "        \n",
                "        if self.max_depth is not None and depth >= self.max_depth:\n",
                "            return Node(label=Counter(y).most_common(1)[0][0])\n",
                "        \n",
                "        # 2. Feature Selection (Random Forest modification)\n",
                "        n_features = len(X[0])\n",
                "        if self.feature_subset_size:\n",
                "            features_indices = random.sample(range(n_features), min(self.feature_subset_size, n_features))\n",
                "        else:\n",
                "            features_indices = range(n_features)\n",
                "\n",
                "        # 3. Find best feature among selected subset\n",
                "        best_feature = None\n",
                "        best_gain = -1\n",
                "\n",
                "        for i in features_indices:\n",
                "            gain = information_gain(X, y, i)\n",
                "            if gain > best_gain:\n",
                "                best_gain = gain\n",
                "                best_feature = i\n",
                "        \n",
                "        if best_gain == 0:\n",
                "             return Node(label=Counter(y).most_common(1)[0][0])\n",
                "\n",
                "        node = Node(feature=best_feature)\n",
                "        \n",
                "        feature_values = set(row[best_feature] for row in X)\n",
                "        for value in feature_values:\n",
                "            sub_X = []\n",
                "            sub_y = []\n",
                "            for i in range(len(X)):\n",
                "                if X[i][best_feature] == value:\n",
                "                    sub_X.append(X[i])\n",
                "                    sub_y.append(y[i])\n",
                "            \n",
                "            node.children[value] = self._build_tree(\n",
                "                sub_X, sub_y, depth + 1\n",
                "            )\n",
                "            \n",
                "        return node\n",
                "\n",
                "    def predict_one(self, x, node):\n",
                "        if node.label is not None:\n",
                "            return node.label\n",
                "        \n",
                "        value = x[node.feature]\n",
                "        child = node.children.get(value)\n",
                "        \n",
                "        if child is None:\n",
                "            # Fallback for unseen value path: return most common or random\n",
                "            # Simple fix: return None or handle appropriately. \n",
                "            # Here we just return None which might fail aggregation if not handled.\n",
                "            return None \n",
                "        \n",
                "        return self.predict_one(x, child)\n",
                "\n",
                "    def predict(self, X):\n",
                "        return [self.predict_one(x, self.root) for x in X]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Random Forest Implementation\n",
                "\n",
                "Key components:\n",
                "1.  **Bootstrapping**: Train each tree on a random sample of the data with replacement.\n",
                "2.  **Feature Subsampling**: At each split, consider only a random subset of features (implemented inside `DecisionTree` above).\n",
                "3.  **Aggregation**: Majority vote for classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RandomForest:\n",
                "    def __init__(self, n_trees=10, max_depth=None, feature_subset_size=None):\n",
                "        self.n_trees = n_trees\n",
                "        self.max_depth = max_depth\n",
                "        self.feature_subset_size = feature_subset_size\n",
                "        self.trees = []\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        self.trees = []\n",
                "        n_samples = len(X)\n",
                "        for _ in range(self.n_trees):\n",
                "            # Bootstrapping\n",
                "            indices = [random.randint(0, n_samples - 1) for _ in range(n_samples)]\n",
                "            sample_X = [X[i] for i in indices]\n",
                "            sample_y = [y[i] for i in indices]\n",
                "            \n",
                "            tree_model = DecisionTree(\n",
                "                max_depth=self.max_depth, \n",
                "                feature_subset_size=self.feature_subset_size\n",
                "            )\n",
                "            tree_model.fit(sample_X, sample_y)\n",
                "            self.trees.append(tree_model)\n",
                "\n",
                "    def predict(self, X):\n",
                "        # Collect predictions from all trees\n",
                "        tree_predictions = []\n",
                "        for tree_model in self.trees:\n",
                "            pred = tree_model.predict(X)\n",
                "            tree_predictions.append(pred)\n",
                "        \n",
                "        # Transpose to get predictions per sample: [[p1_t1, p1_t2...], [p2_t1, ...]]\n",
                "        # Only works if all predictions are valid (not None). \n",
                "        # We filter out Nones if any tree couldn't predict.\n",
                "        \n",
                "        final_predictions = []\n",
                "        n_samples = len(X)\n",
                "        \n",
                "        for i in range(n_samples):\n",
                "            sample_votes = []\n",
                "            for j in range(self.n_trees):\n",
                "                vote = tree_predictions[j][i]\n",
                "                if vote is not None:\n",
                "                    sample_votes.append(vote)\n",
                "            \n",
                "            if len(sample_votes) > 0:\n",
                "                # Majority vote\n",
                "                final_predictions.append(Counter(sample_votes).most_common(1)[0][0])\n",
                "            else:\n",
                "                final_predictions.append(None) # Should not happen typically\n",
                "                \n",
                "        return final_predictions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Testing Random Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Predictions: ['Yes', 'No']\n"
                    ]
                }
            ],
            "source": [
                "# Example dataset (Weather Data)\n",
                "X_train = [\n",
                "    ['Sunny', 'Hot', 'High', 'Weak'],\n",
                "    ['Sunny', 'Hot', 'High', 'Strong'],\n",
                "    ['Overcast', 'Hot', 'High', 'Weak'],\n",
                "    ['Rain', 'Mild', 'High', 'Weak'],\n",
                "    ['Rain', 'Cool', 'Normal', 'Weak'],\n",
                "    ['Rain', 'Cool', 'Normal', 'Strong'],\n",
                "    ['Overcast', 'Cool', 'Normal', 'Strong'],\n",
                "    ['Sunny', 'Mild', 'High', 'Weak'],\n",
                "    ['Sunny', 'Cool', 'Normal', 'Weak'],\n",
                "    ['Rain', 'Mild', 'Normal', 'Weak'],\n",
                "    ['Sunny', 'Mild', 'Normal', 'Strong'],\n",
                "    ['Overcast', 'Mild', 'High', 'Strong'],\n",
                "    ['Overcast', 'Hot', 'Normal', 'Weak'],\n",
                "    ['Rain', 'Mild', 'High', 'Strong']\n",
                "]\n",
                "\n",
                "y_train = ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
                "\n",
                "# Test Data\n",
                "X_test = [\n",
                "    ['Sunny', 'Cool', 'Normal', 'Weak'], # Should be Yes\n",
                "    ['Rain', 'Mild', 'High', 'Strong']   # Should be No\n",
                "]\n",
                "\n",
                "# Initialize Random Forest\n",
                "rf = RandomForest(n_trees=5, max_depth=5, feature_subset_size=2)\n",
                "\n",
                "# Train\n",
                "rf.fit(X_train, y_train)\n",
                "\n",
                "# Predict\n",
                "predictions = rf.predict(X_test)\n",
                "print(\"Predictions:\", predictions)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Compare with Single Decision Tree"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Single Tree Predictions: ['Yes', 'No']\n"
                    ]
                }
            ],
            "source": [
                "dt = DecisionTree(max_depth=5)\n",
                "dt.fit(X_train, y_train)\n",
                "dt_preds = dt.predict(X_test)\n",
                "print(\"Single Tree Predictions:\", dt_preds)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
